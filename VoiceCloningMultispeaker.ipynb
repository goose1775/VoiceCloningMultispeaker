{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install --upgrade pip setuptools wheel  \n",
    "#!pip install numpy\n",
    "\n",
    "#!pip install ffmpeg-python\n",
    "\n",
    "#!pip install pydub\n",
    "#!pip install demucs\n",
    "#!pip install diffq\n",
    "\n",
    "#!pip install ipywidgets \n",
    "#!pip install soundfile\n",
    "#!pip install pyannote.audio\n",
    "#!pip install openai-whisper\n",
    "#!pip install scikit-learn\n",
    "#!pip install spacy==3.4.4\n",
    "#!pip install Cython==0.29.30 ou Cython==0.29.21\n",
    "#!pip install numpy==1.26.2\n",
    "#!pip install librosa\n",
    "#!pip install TTS\n",
    "#!pip install moviepy\n",
    "\n",
    "#Working so far\n",
    "#Versão do TTS: 0.22.0\n",
    "#Versão do spacy: 3.4.4\n",
    "#Versão do Cython: 0.29.30\n",
    "#Versão do numpy: 1.22.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar GPU\n",
    "import torch\n",
    "print(\"GPU disponível:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Célula 1 - Extração e Preparação do Áudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extração de legendas de arquivos .mkv com Python\n",
    "import subprocess\n",
    "\n",
    "# Path do arquivo .mkv e nome do episódio\n",
    "episode_path = \"\"\n",
    "episode_name = \"\"\n",
    "input_file = f\"{episode_path}/{episode_name}.mkv\"\n",
    "output_path = f\"{episode_path}/{episode_name}.srt\"\n",
    "\n",
    "# Extração de legendas de arquivos .mkv com Python\n",
    "subtitle_stream_id = \"0:3\"  # Stream ID da legenda desejada (português)\n",
    "\n",
    "# Função para listar as faixas de um arquivo .mkv\n",
    "def list_mkv_tracks(file_path):\n",
    "    try:\n",
    "        command = [\"ffmpeg\", \"-i\", file_path]\n",
    "        process = subprocess.run(command, stderr=subprocess.PIPE, text=True)\n",
    "        print(\"Faixas disponíveis no arquivo:\")\n",
    "        print(process.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao listar faixas: {e}\")\n",
    "\n",
    "# Função para extrair e converter a legenda para .srt\n",
    "def extract_subtitle_as_srt(file_path, stream_id, output_file):\n",
    "    try:\n",
    "        command = [\n",
    "            \"ffmpeg\", \"-i\", file_path, \n",
    "            \"-map\", stream_id, \n",
    "            \"-c:s\", \"srt\",  # Converte para formato .srt\n",
    "            output_file\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Legenda extraída e convertida para .srt: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair e converter legenda: {e}\")\n",
    "\n",
    "# Chamada das funções\n",
    "#list_mkv_tracks(input_file)  # Listar faixas\n",
    "extract_subtitle_as_srt(input_file, subtitle_stream_id, output_path)  # Extrair e converter para .srt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 - Extração e Preparação do Áudio\n",
    "# Esta célula extrai o áudio de um arquivo de vídeo (episódio de anime) e o converte para um formato padronizado.\n",
    "# Requer a instalação do FFmpeg para manipulação de mídia.\n",
    "\n",
    "# Comando para instalar o FFmpeg Python wrapper caso ainda não esteja instalado\n",
    "# pip install ffmpeg-python\n",
    "\n",
    "import ffmpeg\n",
    "import os\n",
    "\n",
    "def extract_and_convert_audio(video_path, output_dir, audio_format=\"wav\", sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Extrai o áudio de um vídeo, converte para o formato especificado e salva no diretório de saída.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Caminho para o arquivo de vídeo.\n",
    "        output_dir (str): Caminho para salvar o áudio extraído.\n",
    "        audio_format (str): Formato do áudio de saída (padrão: \"wav\").\n",
    "        sample_rate (int): Taxa de amostragem do áudio (padrão: 16kHz).\n",
    "\n",
    "    Returns:\n",
    "        str: Caminho para o arquivo de áudio gerado.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    audio_output_path = os.path.join(\n",
    "        output_dir, f\"{os.path.splitext(os.path.basename(video_path))[0]}.{audio_format}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Extrai e converte o áudio\n",
    "        ffmpeg.input(video_path).output(\n",
    "            audio_output_path, \n",
    "            format=audio_format, \n",
    "            ar=sample_rate, \n",
    "            ac=1  # Mono\n",
    "        ).run(overwrite_output=True)\n",
    "        print(f\"Áudio extraído e salvo em: {audio_output_path}\")\n",
    "        return audio_output_path\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Erro ao processar o vídeo:\", e)\n",
    "        return None\n",
    "\n",
    "def verify_audio_properties(audio_path):\n",
    "    \"\"\"\n",
    "    Verifica as propriedades de um arquivo de áudio para garantir que estão conforme o esperado.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Caminho para o arquivo de áudio.\n",
    "\n",
    "    Returns:\n",
    "        dict: Propriedades do arquivo de áudio, como taxa de amostragem, canais e duração.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        probe = ffmpeg.probe(audio_path)\n",
    "        audio_stream = next(stream for stream in probe[\"streams\"] if stream[\"codec_type\"] == \"audio\")\n",
    "        properties = {\n",
    "            \"sample_rate\": int(audio_stream[\"sample_rate\"]),\n",
    "            \"channels\": int(audio_stream[\"channels\"]),\n",
    "            \"duration\": float(audio_stream[\"duration\"])\n",
    "        }\n",
    "        return properties\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Erro ao verificar propriedades do áudio:\", e)\n",
    "        return None\n",
    "\n",
    "# Exemplo de uso:\n",
    "video_path = f\"{episode_path}/{episode_name}.mkv\"\n",
    "output_dir = f\"{episode_path}/{episode_name}_audio\"\n",
    "\n",
    "audio_path = extract_and_convert_audio(video_path, output_dir)\n",
    "print('Propriedades do áudio:', verify_audio_properties(audio_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Célula 2 - Separacão de Fontes (Remoção de Ruídos e Música): demucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 2.1 - Hyperparâmetros e Configuração demucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2.1 - Hyperparâmetros e Configuração\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from datetime import datetime\n",
    "\n",
    "def current_time():\n",
    "    \"\"\"Retorna o horário atual no formato HH:MM:SS.\"\"\"\n",
    "    return datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "def evaluate_output_quality(output_dir):\n",
    "    \"\"\"\n",
    "    Avalia a qualidade dos arquivos de áudio gerados pela separação do Demucs.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Caminho para o diretório contendo os arquivos separados.\n",
    "\n",
    "    Returns:\n",
    "        float: Uma métrica de qualidade (maior é melhor).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[{current_time()}] Iniciando avaliação de qualidade no diretório: {output_dir}\")\n",
    "        # Inicializa a qualidade total\n",
    "        total_quality = 0\n",
    "        count = 0\n",
    "\n",
    "        for root, _, files in os.walk(output_dir):  # Verifica também subdiretórios\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(f\"Processando arquivo: {file_path}\")\n",
    "                    audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "                    # Métrica baseada na relação sinal-ruído (SNR)\n",
    "                    samples = np.array(audio.get_array_of_samples())\n",
    "                    signal_power = np.mean(samples**2)\n",
    "                    noise_power = np.var(samples)\n",
    "\n",
    "                    if noise_power > 0:\n",
    "                        snr = 10 * np.log10(signal_power / noise_power)\n",
    "                    else:\n",
    "                        snr = float('inf')\n",
    "\n",
    "                    print(f\"Arquivo: {file}, SNR calculado: {snr}\\n\")\n",
    "\n",
    "                    # Incrementa qualidade total\n",
    "                    total_quality += snr\n",
    "                    count += 1\n",
    "\n",
    "        # Retorna a média da qualidade\n",
    "        if count > 0:\n",
    "            average_quality = total_quality / count\n",
    "            print(f\"Média de qualidade calculada: {average_quality}\")\n",
    "            return average_quality\n",
    "        else:\n",
    "            print(\"Nenhum arquivo de áudio encontrado para avaliação.\")\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao avaliar a qualidade no diretório {output_dir}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def optimize_demucs(audio_path, output_dir, models, segments, devices):\n",
    "    best_config = None\n",
    "    best_quality = float('-inf')\n",
    "\n",
    "    print(f\"[{current_time()}] Iniciando otimização do Demucs...\")\n",
    "    for model in models:\n",
    "        print(f\"Testando modelo: {model}\")\n",
    "        for segment in segments:\n",
    "            print(f\"  Tamanho de segmento: {segment}s\")\n",
    "            for device in devices:\n",
    "                print(f\"    Dispositivo: {device}\")\n",
    "                output_path = f\"{output_dir}/{model}_{segment}_{device}\"\n",
    "\n",
    "                # Garante que o diretório existe\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "                # Comando para executar o Demucs\n",
    "                command = [\n",
    "                    \"demucs\", audio_path, \"--out\", output_path,\n",
    "                    \"-n\", model, \"--device\", device,\n",
    "                    \"--segment\", str(segment)\n",
    "                ]\n",
    "                \n",
    "                try:\n",
    "                    print(f\"[{current_time()}] Executando comando: {' '.join(command)}\")\n",
    "                    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                    print(f\"Saída do comando: {result.stdout.decode('utf-8')}\")\n",
    "\n",
    "                    # Avalia a qualidade do resultado\n",
    "                    quality = evaluate_output_quality(output_path)\n",
    "                    print(f\"Qualidade obtida: {quality}\")\n",
    "\n",
    "                    if quality > best_quality:\n",
    "                        print(f\"*** Nova melhor configuração encontrada! {model}, {segment}, {device} com qualidade {quality} ***\")\n",
    "                        best_quality = quality\n",
    "                        best_config = (model, segment, device)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao executar o comando: {e}\")\n",
    "\n",
    "    print(f\"Melhor configuração: {best_config} com qualidade {best_quality}\")\n",
    "    return best_config\n",
    "\n",
    "# Configurações de exemplo\n",
    "audio_path = f\"{episode_path}/{episode_name}_audio\"\n",
    "output_dir = f\"{episode_path}data/audio_file\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "models = [\"mdx_extra_q\"]#, \"htdemucs\"]\n",
    "segments = [5, 10, 20]\n",
    "devices = [\"cuda\"]\n",
    "\n",
    "best_config = optimize_demucs(audio_path, output_dir, models, segments, devices)\n",
    "print(f\"A melhor configuração foi: {best_config}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 2.2 - Separacão de Canais de áudio - Sem Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2.2 - Separacão de Fontes (Remoção de Ruídos e Música)\n",
    "# Esta célula utiliza o Demucs para separar a trilha de diálogo das outras fontes, como música e efeitos sonoros.\n",
    "# Requer a instalação do Demucs para realizar a separação de fontes.\n",
    "\n",
    "# Comando para instalar o Demucs caso ainda não esteja instalado\n",
    "# Para GPUs modernas, instale o PyTorch com suporte a CUDA mais recente:\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu181\n",
    "# pip install demucs\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def separate_sources(audio_path, output_dir, demucs_model, segment, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Separa o áudio em diferentes fontes (diálogo, música, efeitos sonoros) usando o Demucs.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Caminho para o arquivo de áudio a ser processado.\n",
    "        output_dir (str): Diretório onde os arquivos de saída serão salvos.\n",
    "        demucs_model (str): Nome do modelo Demucs a ser utilizado.\n",
    "        use_gpu (bool): Se True, usa GPU para acelerar o processamento (requer CUDA).\n",
    "\n",
    "    Returns:\n",
    "        str: Caminho para o arquivo contendo apenas o diálogo, ou None em caso de erro.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        # Argumentos para Demucs\n",
    "        demucs_command = [\"demucs\", audio_path, \"--out\", output_dir, \"-n\", demucs_model, \"--segment\", str(segment)]\n",
    "        if use_gpu:\n",
    "            demucs_command.append(\"--device=cuda\")\n",
    "\n",
    "        # Executa o Demucs para separar as fontes\n",
    "        result = subprocess.run(demucs_command, check=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding=\"utf-8\")\n",
    "\n",
    "        # Log detalhado\n",
    "        print(\"Saída do Demucs:\")\n",
    "        print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"Erros do Demucs:\")\n",
    "            print(result.stderr)\n",
    "\n",
    "        # Caminho do diálogo separado\n",
    "        vocals_path = os.path.join(output_dir, demucs_model, os.path.splitext(os.path.basename(audio_path))[0], \"vocals.wav\")\n",
    "        if os.path.exists(vocals_path):\n",
    "            print(f\"Separação de fontes concluída. Diálogo salvo em: {vocals_path}\")\n",
    "            return vocals_path\n",
    "        else:\n",
    "            print(\"Não foi possível encontrar o arquivo de diálogo separado.\")\n",
    "            return None\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Erro ao executar o Demucs: {e}\")\n",
    "        print(\"Saída padrão (stdout):\", e.stdout)\n",
    "        print(\"Erro padrão (stderr):\", e.stderr)\n",
    "        return None\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Erro de decodificação: {e}\")\n",
    "        return None\n",
    "\n",
    "# Exemplo de uso:\n",
    "demucs_model = \"mdx_extra_q\"  # htdemucs ou mdx_extra_q\n",
    "audio_path = f\"{episode_path}/{episode_name}_audio.wav\"\n",
    "output_dir = f\"{episode_path}/{episode_name}/audio_file\"\n",
    "\n",
    "dialogue_path = separate_sources(audio_path, output_dir, demucs_model, segment=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 2.3- Verificar Energia dos Segmentos de Áudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2.3 - Verificar Energia dos Segmentos de Áudio\n",
    "# Esta célula analisa os arquivos segmentados pelo Demucs para identificar trechos com silêncio ou baixa energia.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import wave\n",
    "\n",
    "def calculate_audio_energy(audio_path):\n",
    "    \"\"\"\n",
    "    Calcula a energia de um arquivo de áudio.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Caminho para o arquivo de áudio.\n",
    "\n",
    "    Returns:\n",
    "        float: Energia média do áudio.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with wave.open(audio_path, 'r') as wav_file:\n",
    "            n_frames = wav_file.getnframes()\n",
    "            audio_data = np.frombuffer(wav_file.readframes(n_frames), dtype=np.int16)\n",
    "            energy = np.mean(audio_data**2)\n",
    "        return energy\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao calcular energia do áudio {audio_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def diagnose_audio_segments(audio_dir, energy_threshold=1e4):\n",
    "    \"\"\"\n",
    "    Diagnostica os arquivos de áudio segmentados pelo Demucs, verificando energia.\n",
    "\n",
    "    Args:\n",
    "        audio_dir (str): Diretório contendo os arquivos segmentados.\n",
    "        energy_threshold (float): Limite mínimo de energia para considerar um segmento válido.\n",
    "\n",
    "    Returns:\n",
    "        dict: Resultados do diagnóstico com energia de cada arquivo.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for audio_file in os.listdir(audio_dir):\n",
    "        if audio_file.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(audio_dir, audio_file)\n",
    "            energy = calculate_audio_energy(audio_path)\n",
    "            results[audio_file] = energy\n",
    "\n",
    "            if energy < energy_threshold:\n",
    "                print(f\"Segmento {audio_file} possui baixa energia ({energy:.2f}). Pode conter silêncio.\")\n",
    "            else:\n",
    "                print(f\"Segmento {audio_file} possui energia válida ({energy:.2f}).\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exemplo de uso:\n",
    "audio_dir= f\"{episode_path}/{episode_name}/audio_file\"\n",
    "print(\"audio_dir:\", audio_dir)\n",
    "segment_energy_results = diagnose_audio_segments(\n",
    "    audio_dir,\n",
    "    energy_threshold=1e4\n",
    ")\n",
    "\n",
    "print(\"Diagnóstico de energia concluído.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Célula 3 - Diarização e Identificação de Locutores: pyannote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 3.1 - Diarização e Identificação de Locutores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização de energia\n",
    "# from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def normalize_energy(audio_path, output_path):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "    normalized_samples = samples / np.max(np.abs(samples))  # Normalização\n",
    "    normalized_audio = audio._spawn(normalized_samples.astype(np.int16).tobytes())\n",
    "    normalized_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Áudio normalizado salvo em: {output_path}\")\n",
    "\n",
    "normalize_energy(audio_path, \"data/audio_file/normalized_vocals.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3.1 - Diarização e Identificação de Locutores\n",
    "# Esta célula utiliza o pyannote-audio para identificar diferentes locutores em um arquivo de áudio.\n",
    "# Requer a instalação do pyannote-audio e modelos correspondentes.\n",
    "\n",
    "# Comando para instalação:\n",
    "# pip install pyannote.audio\n",
    "# pip install numpy==1.26.2\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "import torchaudio\n",
    "\n",
    "def diarize_speakers(audio_path, model_name=\"pyannote/speaker-diarization-3.1\", hf_token=None): \n",
    "    \"\"\"\n",
    "    Realiza diarização para identificar diferentes locutores em um áudio, utilizando GPU, memória pré-carregada e monitorando progresso.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Caminho para o arquivo de áudio.\n",
    "        model_name (str): Nome do modelo do pyannote para diarização.\n",
    "        hf_token (str): Token de autenticação do Hugging Face.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de segmentos com identificação de locutores e intervalos de tempo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not hf_token:\n",
    "            raise ValueError(\"Token de autenticação do Hugging Face é necessário.\")\n",
    "\n",
    "        # Carrega o pipeline com autenticação\n",
    "        pipeline = Pipeline.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "\n",
    "        # Configura para usar GPU\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Usando dispositivo: {device}\")\n",
    "        pipeline.to(device)\n",
    "        \n",
    "        # Pré-carrega o áudio em memória\n",
    "        print(\"Carregando o áudio...\")\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        print(f\"Shape do waveform: {waveform.shape}, Taxa de amostragem: {sample_rate}\")\n",
    "\n",
    "        # Reamostrar se necessário\n",
    "        if sample_rate != 16000:\n",
    "            print(f\"Reamostrando o áudio para 16 kHz...\")\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "            sample_rate = 16000\n",
    "\n",
    "        # Passa o áudio ao pipeline\n",
    "        audio_input = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "        # Monitora o progresso do pipeline\n",
    "        with ProgressHook() as hook:\n",
    "            diarization = pipeline(audio_input, hook=hook)\n",
    "        \n",
    "        segments = []\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            segments.append({\n",
    "                \"start\": turn.start,\n",
    "                \"end\": turn.end,\n",
    "                \"speaker\": speaker\n",
    "            })\n",
    "            print(f\"Locutor {speaker}: {turn.start:.2f}s - {turn.end:.2f}s\")\n",
    "\n",
    "        # Salva a saída em formato RTTM\n",
    "        output_rttm = audio_path.replace(\".wav\", \".rttm\")\n",
    "        with open(output_rttm, \"w\") as rttm:\n",
    "            diarization.write_rttm(rttm)\n",
    "        print(f\"Arquivo RTTM salvo em: {output_rttm}\")\n",
    "\n",
    "        return segments\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao realizar diarização: {e}\")\n",
    "        return None\n",
    "\n",
    "# Exemplo de uso:\n",
    "hf_token = \"\"\n",
    "audio_path = \"\"\n",
    "segments = diarize_speakers(audio_path, hf_token=hf_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 3.2 - Diarização Tunned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3.2 - Diarização e Identificação de Locutores\n",
    "# Esta célula utiliza o pyannote-audio para identificar diferentes locutores em um arquivo de áudio.\n",
    "# Requer a instalação do pyannote-audio e modelos correspondentes.\n",
    "\n",
    "# Comando para instalação:\n",
    "# pip install pyannote.audio\n",
    "# pip install numpy==1.26.2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.pipeline import Optimizer\n",
    "from pyannote.core import Annotation, Segment\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Ignorar Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Função para Obter Horário Atual ----\n",
    "def current_time():\n",
    "    \"\"\"Retorna o horário atual no formato HH:MM:SS.\"\"\"\n",
    "    return datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "# Configura para usar GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[{current_time()}] Usando dispositivo: {device}\")\n",
    "\n",
    "# ---- Função para Carregar RTTM ----\n",
    "def load_rttm(rttm_path):\n",
    "    annotation = Annotation()\n",
    "    with open(rttm_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            start, duration, speaker = float(parts[3]), float(parts[4]), parts[7]\n",
    "            end = start + duration\n",
    "            annotation[Segment(start, end)] = speaker\n",
    "    return annotation\n",
    "\n",
    "# ---- Configuração do Pipeline ----\n",
    "# Realizar login na Hugging Face\n",
    "token = \"\"\n",
    "login(token=token)\n",
    "\n",
    "# Caminhos dos arquivos de entrada e saída\n",
    "audio_file = \"\"\n",
    "annotation_file = \"\" # .rttm\n",
    "optimized_rttm = \"\"\n",
    "\n",
    "# Carregar pipeline pré-treinado\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=True)\n",
    "pipeline.to(device)\n",
    "\n",
    "# ---- Avaliação Inicial da Diarização ----\n",
    "print(f\"\\n[{current_time()}] [INFO] Avaliando DER inicial...\")\n",
    "metric = DiarizationErrorRate()\n",
    "\n",
    "# Carregar ground_truth\n",
    "ground_truth = load_rttm(annotation_file)\n",
    "\n",
    "# Aplicar o pipeline\n",
    "initial_result = pipeline({\"uri\": audio_file, \"audio\": audio_file})\n",
    "\n",
    "initial_der = metric(ground_truth, initial_result)\n",
    "print(f\"[{current_time()}] DER Inicial: {100 * abs(initial_der):.2f}%\")\n",
    "\n",
    "# Exibir valores iniciais de clustering e segmentation do pipeline\n",
    "initial_clustering = pipeline.parameters(instantiated=True).get('clustering', {})\n",
    "initial_segmentation = pipeline.parameters(instantiated=True).get('segmentation', {})\n",
    "print(f\"[{current_time()}] Parâmetros Iniciais - Clustering: {initial_clustering}, Segmentation: {initial_segmentation}\")\n",
    "\n",
    "# ---- Otimização dos Hiperparâmetros ----\n",
    "print(f\"\\n[{current_time()}] [INFO] Otimizando Hiperparâmetros de Segmentação...\")\n",
    "optimizer = Optimizer(pipeline)\n",
    "\n",
    "# Atualizar dataset com áudio e ground truth\n",
    "dataset = [{\"uri\": audio_file, \"audio\": audio_file, \"annotation\": ground_truth}]\n",
    "\n",
    "# Configuração dos limites\n",
    "n_iter = 10  # Número máximo de iterações\n",
    "patience = 3  # Número de iterações sem melhora\n",
    "\n",
    "# Extrair valores específicos de clustering e segmentation\n",
    "initial_clustering_threshold = initial_clustering.get('threshold', 0.8)  # Valor padrão caso não exista\n",
    "initial_segmentation_min_duration_off = initial_segmentation.get('min_duration_off', 0.01)\n",
    "\n",
    "# Atribuir valores específicos ao warm_start_params\n",
    "warm_start_params = {\n",
    "    \"segmentation\": {\"min_duration_off\": initial_segmentation_min_duration_off},\n",
    "    \"clustering\": {\"threshold\": initial_clustering_threshold}\n",
    "}\n",
    "\n",
    "# ---- Otimização de segmentation.min_duration_off ----\n",
    "best_der = float(\"inf\")\n",
    "no_improvement_count = 0\n",
    "best_segmentation_threshold = None\n",
    "\n",
    "print(f\"[{current_time()}] Parâmetros de WarmUp: {warm_start_params}\")\n",
    "\n",
    "iterations_seg = optimizer.tune_iter(dataset, warm_start=warm_start_params, show_progress=True)\n",
    "for i, iteration in enumerate(iterations_seg):\n",
    "    current_threshold = iteration['params'].get('segmentation', {}).get('min_duration_off')\n",
    "    if current_threshold is not None:\n",
    "        pipeline.instantiate({\"segmentation\": {\"min_duration_off\": current_threshold}})\n",
    "        result = pipeline({\"uri\": audio_file, \"audio\": audio_file})\n",
    "        current_der = metric(ground_truth, result)\n",
    "        \n",
    "        print(f\"[{current_time()}] Iteração {i}: DER = {100 * abs(current_der):.2f}%, min_duration_off = {current_threshold}\")\n",
    "\n",
    "        if current_der < best_der:\n",
    "            best_der = current_der\n",
    "            best_segmentation_threshold = current_threshold\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"[{current_time()}] Nenhuma melhora em {patience} iterações. Parando otimização de segmentação.\")\n",
    "            break\n",
    "\n",
    "    if i >= n_iter:\n",
    "        print(f\"[{current_time()}] Número máximo de {n_iter} iterações atingido.\")\n",
    "        break\n",
    "\n",
    "# Atualizar pipeline com segmentation.min_duration_off otimizado\n",
    "pipeline.instantiate({\"segmentation\": {\"min_duration_off\": best_segmentation_threshold}})\n",
    "\n",
    "# ---- Otimização de clustering.threshold ----\n",
    "best_clustering_threshold = None\n",
    "no_improvement_count = 0\n",
    "\n",
    "print(f\"\\n[{current_time()}] [INFO] Otimizando Hiperparâmetros de Clusterização...\")\n",
    "iterations_clust = optimizer.tune_iter(dataset, warm_start=warm_start_params, show_progress=True)\n",
    "for i, iteration in enumerate(iterations_clust):\n",
    "    current_threshold = iteration['params'].get('clustering', {}).get('threshold')\n",
    "    if current_threshold is not None:\n",
    "        pipeline.instantiate({\"clustering\": {\"threshold\": current_threshold}})\n",
    "        result = pipeline({\"uri\": audio_file, \"audio\": audio_file})\n",
    "        current_der = metric(ground_truth, result)\n",
    "        \n",
    "        print(f\"[{current_time()}] Iteração {i}: DER = {100 * abs(current_der):.2f}%, threshold = {current_threshold}\")\n",
    "\n",
    "        if current_der < best_der:\n",
    "            best_der = current_der\n",
    "            best_clustering_threshold = current_threshold\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"[{current_time()}] Nenhuma melhora em {patience} iterações. Parando otimização de clusterização.\")\n",
    "            break\n",
    "\n",
    "    if i >= n_iter:\n",
    "        print(f\"[{current_time()}] Número máximo de {n_iter} iterações atingido.\")\n",
    "        break\n",
    "\n",
    "# Atualizar pipeline com clustering.threshold otimizado\n",
    "pipeline.instantiate({\n",
    "    \"segmentation\": {\"min_duration_off\": best_segmentation_threshold},\n",
    "    \"clustering\": {\"threshold\": best_clustering_threshold}\n",
    "})\n",
    "\n",
    "# ---- Avaliação Final da Diarização ----\n",
    "print(f\"\\n[{current_time()}] [INFO] Avaliando DER final após otimização...\")\n",
    "optimized_result = pipeline({\"uri\": audio_file, \"audio\": audio_file})\n",
    "final_der = metric(ground_truth, optimized_result)\n",
    "print(f\"[{current_time()}] DER Final: {100 * abs(final_der):.2f}%\")\n",
    "\n",
    "# ---- Comparação e Salvar Resultados ----\n",
    "print(f\"\\n[{current_time()}] [RESULTADOS] Comparação de DER:\")\n",
    "print(f\"[{current_time()}] DER Inicial: {100 * abs(initial_der):.2f}%\")\n",
    "print(f\"[{current_time()}] DER Final após Otimização: {100 * abs(final_der):.2f}%\")\n",
    "\n",
    "# Salvar o resultado otimizado em RTTM\n",
    "with open(optimized_rttm, \"w\") as f:\n",
    "    for turn, _, speaker in optimized_result.itertracks(yield_label=True):\n",
    "        f.write(f\"SPEAKER {audio_file} 1 {turn.start:.3f} {turn.duration:.3f} <NA> <NA> {speaker} <NA>\\n\")\n",
    "print(f\"[{current_time()}] RTTM otimizado salvo em: {optimized_rttm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferenças entre os arquivos RTTM\n",
    "def compare_rttm_files(original_rttm, new_rttm):\n",
    "    \"\"\"\n",
    "    Compara dois arquivos RTTM e exibe as diferenças de rótulos de locutores.\n",
    "\n",
    "    Args:\n",
    "        original_rttm (str): Caminho para o RTTM original.\n",
    "        new_rttm (str): Caminho para o RTTM reorganizado.\n",
    "    \"\"\"\n",
    "    with open(original_rttm, \"r\") as orig, open(new_rttm, \"r\") as new:\n",
    "        original_lines = [line.strip().split() for line in orig.readlines()]\n",
    "        new_lines = [line.strip().split() for line in new.readlines()]\n",
    "\n",
    "    print(\"Comparação de Locutores (Original vs Novo):\\n\")\n",
    "    for i, (orig_line, new_line) in enumerate(zip(original_lines, new_lines)):\n",
    "        orig_speaker = orig_line[7]\n",
    "        new_speaker = new_line[7]\n",
    "        if orig_speaker != new_speaker:\n",
    "            print(f\"Linha {i+1}: {orig_speaker} -> {new_speaker}\")\n",
    "    print(\"\\nComparação concluída.\")\n",
    "\n",
    "# Caminhos dos arquivos RTTM\n",
    "original_rttm = \"\"\n",
    "new_rttm = \"\"\n",
    "\n",
    "# Comparar os arquivos\n",
    "compare_rttm_files(original_rttm, new_rttm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Céula 3.3 - Diarização sob Legendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#configurar print_now\n",
    "def print_now(message):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "# Configurações do episódio\n",
    "# Caminhos dos arquivos\n",
    "base_path = os.path.join(episode_path, episode_name, \"audio_file\", \"mdx_extra_q\", episode_name)\n",
    "# Lengenda Original\n",
    "subtitle_path = os.path.join(episode_path, episode_name, \"audio_file\",  f\"{episode_name}.srt\")\n",
    "# Áudio de Vocais\n",
    "vocals_path = os.path.join(base_path, \"vocals.wav\")\n",
    "# Saída da diarização em RTTM\n",
    "rttm_output_path = os.path.join(base_path, \"diarization_output.rttm\")\n",
    "\n",
    "# Pipeline de diarização\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=hf_token)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline.to(device)\n",
    "\n",
    "# Executa a diarização no áudio de vocais\n",
    "print_now(\"Iniciando a diarização...\")\n",
    "diarization = pipeline(vocals_path)\n",
    "\n",
    "# Salvar saída em formato RTTM\n",
    "with open(rttm_output_path, \"w\") as rttm_file:\n",
    "    diarization.write_rttm(rttm_file)\n",
    "print_now(f\"Diarização salva em: {rttm_output_path}\")\n",
    "\n",
    "# Carregar legendas traduzidas\n",
    "#subtitle_path = os.path.join(base_path, \"legendado.srt\")\n",
    "with open(subtitle_path, \"r\", encoding=\"utf-8\") as srt_file:\n",
    "    subtitles = srt_file.read()\n",
    "\n",
    "# Processamento das legendas para JSON\n",
    "import re\n",
    "subtitle_pattern = re.compile(r\"(\\d+)\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.+?)\\n\\n\", re.DOTALL)\n",
    "matches = subtitle_pattern.findall(subtitles)\n",
    "\n",
    "subtitle_data = []\n",
    "for match in matches:\n",
    "    subtitle_data.append({\n",
    "        \"id\": int(match[0]),\n",
    "        \"start_time\": match[1],\n",
    "        \"end_time\": match[2],\n",
    "        \"text\": re.sub(r\"<.*?>\", \"\", match[3]).strip()\n",
    "    })\n",
    "\n",
    "subtitle_json_path = os.path.join(base_path, \"subtitles.json\")\n",
    "with open(subtitle_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(subtitle_data, json_file, indent=4, ensure_ascii=False)\n",
    "print_now(f\"Subtítulos processados e salvos em: {subtitle_json_path}\")\n",
    "\n",
    "# Alinhamento automático entre RTTM e legendas\n",
    "from pyannote.core import Segment\n",
    "from datetime import timedelta\n",
    "\n",
    "def time_to_seconds(timestamp):\n",
    "    h, m, s = map(float, timestamp.replace(\",\", \".\").split(\":\"))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "aligned_data = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    turn_segment = turn\n",
    "    for subtitle in subtitle_data:\n",
    "        start_time = time_to_seconds(subtitle[\"start_time\"])\n",
    "        end_time = time_to_seconds(subtitle[\"end_time\"])\n",
    "        subtitle_segment = Segment(start=start_time, end=end_time)\n",
    "        if turn_segment.intersects(subtitle_segment):\n",
    "            aligned_data.append({\n",
    "                \"speaker\": speaker,\n",
    "                \"start\": turn_segment.start,\n",
    "                \"end\": turn_segment.end,\n",
    "                \"transcription\": subtitle[\"text\"],\n",
    "                \"duration\": turn_segment.end - turn_segment.start\n",
    "            })\n",
    "\n",
    "# Salvar alinhamento em JSON\n",
    "aligned_json_path = os.path.join(base_path, \"aligned_transcriptions.json\")\n",
    "with open(aligned_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(aligned_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print_now(f\"Alinhamento concluído e salvo em: {aligned_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celula 3.4 - Alinhamento de Legendas e Locutores(seguimentos originais)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para validar tempos e duplicatas e salvar JSON final\n",
    "def validate_and_save_aligned_transcriptions(json_path, output_json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    seen = set()\n",
    "    valid_segments = []\n",
    "    duplicate_segments = []\n",
    "\n",
    "    for segment in data:\n",
    "        key = (segment['start'], segment['end'])\n",
    "        if key in seen:\n",
    "            duplicate_segments.append(segment)\n",
    "        else:\n",
    "            seen.add(key)\n",
    "            valid_segments.append(segment)\n",
    "\n",
    "    # Salvar os segmentos válidos em um novo JSON\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(valid_segments, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return valid_segments, duplicate_segments\n",
    "\n",
    "# Caminho do novo JSON final\n",
    "aligned_json_final_path = os.path.join(base_path, \"aligned_transcriptions_final.json\")\n",
    "\n",
    "# Validar e salvar o JSON final\n",
    "valid_segments, duplicate_segments = validate_and_save_aligned_transcriptions(\n",
    "    aligned_json_path, aligned_json_final_path\n",
    ")\n",
    "\n",
    "# Exibir resumo\n",
    "print(f\"Segmentos válidos: {len(valid_segments)}\")\n",
    "print(f\"Segmentos duplicados: {len(duplicate_segments)}\")\n",
    "print(f\"Arquivo final salvo em: {aligned_json_final_path}\")\n",
    "\n",
    "# Exibir duplicatas para análise manual (opcional)\n",
    "if duplicate_segments:\n",
    "    print(\"\\nSegmentos duplicados encontrados:\")\n",
    "    for seg in duplicate_segments:\n",
    "        print(seg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 3.4 - Alinhamento de Legendas e Locutores(seguimentos originais)\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Função para limpar formatações HTML e extrair apenas texto\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"<[^>]*>\", \"\", text).strip()\n",
    "\n",
    "#configurar print_now\n",
    "def print_now(message):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "# Função para converter tempo de \"hh:mm:ss,ms\" para segundos com formato .3f\n",
    "def convert_time_to_seconds(time_str):\n",
    "    time_parts = re.split(r'[:,]', time_str)\n",
    "    if len(time_parts) != 4:\n",
    "        raise ValueError(f\"Formato de tempo inesperado: {time_str}\")\n",
    "    hours, minutes, seconds, milliseconds = map(float, time_parts)\n",
    "    return round(hours * 3600 + minutes * 60 + seconds + milliseconds / 1000, 3)\n",
    "\n",
    "# Função para processar legendas em formato SRT\n",
    "def process_srt_to_json(srt_file_path):\n",
    "    with open(srt_file_path, 'r', encoding='utf-8') as srt_file:\n",
    "        lines = srt_file.readlines()\n",
    "\n",
    "    subtitles = []\n",
    "    subtitle = {}\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if re.match(r\"^\\d+$\", line):\n",
    "            if subtitle:\n",
    "                subtitles.append(subtitle)\n",
    "                subtitle = {}\n",
    "            subtitle['id'] = int(line)\n",
    "        elif re.match(r\"^\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}$\", line):\n",
    "            start, end = line.split(' --> ')\n",
    "            subtitle['start_time'] = start\n",
    "            subtitle['end_time'] = end\n",
    "        elif line:\n",
    "            subtitle['text'] = clean_text(line) if 'text' not in subtitle else f\"{subtitle['text']} {clean_text(line)}\"\n",
    "\n",
    "    if subtitle:\n",
    "        subtitles.append(subtitle)\n",
    "\n",
    "    return subtitles\n",
    "\n",
    "# Função para verificar sobreposição excessiva entre locutores e legendas\n",
    "def check_overlap(diarization_segments, subtitles):\n",
    "    validated_segments = []\n",
    "    for subtitle in subtitles:\n",
    "        subtitle_start = convert_time_to_seconds(subtitle['start_time'])\n",
    "        subtitle_end = convert_time_to_seconds(subtitle['end_time'])\n",
    "        overlap_found = False\n",
    "\n",
    "        for segment in diarization_segments:\n",
    "            segment_start = segment['start']\n",
    "            segment_end = segment['end']\n",
    "\n",
    "            if not (segment_end < subtitle_start or segment_start > subtitle_end):\n",
    "                overlap_found = True\n",
    "                validated_segments.append({\n",
    "                    \"speaker\": segment['speaker'],\n",
    "                    \"start\": max(segment_start, subtitle_start),\n",
    "                    \"end\": min(segment_end, subtitle_end),\n",
    "                    \"text\": subtitle['text']\n",
    "                })\n",
    "\n",
    "        if not overlap_found:\n",
    "            print(f\"Legenda sem correspondência: {subtitle}\")\n",
    "\n",
    "    return validated_segments\n",
    "\n",
    "# Função para combinar fragmentos e ajustar os tempos\n",
    "def adjust_fragments(validated_segments):\n",
    "    combined_segments = []\n",
    "    previous_segment = None\n",
    "\n",
    "    for segment in validated_segments:\n",
    "        if previous_segment and previous_segment['speaker'] == segment['speaker'] and \\\n",
    "                abs(segment['start'] - previous_segment['end']) < 0.3:  # Ajuste para combinar fragmentos próximos\n",
    "            previous_segment['end'] = segment['end']\n",
    "            previous_segment['text'] += f\" {segment['text']}\"\n",
    "        else:\n",
    "            if previous_segment:\n",
    "                combined_segments.append(previous_segment)\n",
    "            previous_segment = segment\n",
    "\n",
    "    if previous_segment:\n",
    "        combined_segments.append(previous_segment)\n",
    "\n",
    "    return combined_segments\n",
    "\n",
    "# Função para segmentar áudio usando FFmpeg e atualizar o JSON\n",
    "def segment_audio(audio_file_path, aligned_transcriptions, output_dir, resume=False):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    updated_segments = []\n",
    "\n",
    "    for idx, segment in enumerate(aligned_transcriptions, 1):\n",
    "        output_path = os.path.join(output_dir, f\"{segment['speaker']}_{segment['start']:.3f}_{segment['end']:.3f}.wav\")\n",
    "\n",
    "        # Verifica se o arquivo já foi criado e pula, se necessário\n",
    "        if resume and os.path.exists(output_path):\n",
    "            print_now(f\"[Ignorado] Segmento já existe: {output_path}\")\n",
    "            segment['audio_path'] = output_path\n",
    "            segment['duration'] = round(segment['end'] - segment['start'], 3)\n",
    "            updated_segments.append(segment)\n",
    "            continue\n",
    "\n",
    "        ffmpeg_command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", audio_file_path,\n",
    "            \"-ss\", str(segment['start']),\n",
    "            \"-to\", str(segment['end']),\n",
    "            \"-c\", \"copy\",\n",
    "            output_path\n",
    "        ]\n",
    "        try:\n",
    "            print_now(f\"[Iniciando] Segmento {idx}/{len(aligned_transcriptions)}\")\n",
    "            print(f\"[Comando FFmpeg]: {' '.join(ffmpeg_command)}\")\n",
    "            subprocess.run(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "            print_now(f\"[Concluído] Segmento criado: {output_path}\")\n",
    "            segment['audio_path'] = output_path\n",
    "            segment['duration'] = round(segment['end'] - segment['start'], 3)\n",
    "            updated_segments.append(segment)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print_now(f\"[Erro FFmpeg] Segmento {idx} falhou com erro: {e}\")\n",
    "        except Exception as e:\n",
    "            print_now(f\"[Erro] Falha inesperada no segmento {idx}: {e}\")\n",
    "\n",
    "    return updated_segments\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "episode_path = \"E:/Animes_Raw/[Erai-raws] 3-gatsu no Lion - 01 ~ 22 [1080p][Multiple Subtitle]\"\n",
    "episode_name = \"[Erai-raws] 3-gatsu no Lion - 01 [1080p][Multiple Subtitle][C4EC59D5]\"\n",
    "base_path = os.path.join(episode_path, episode_name, \"audio_file\", \"mdx_extra_q\", episode_name)\n",
    "\n",
    "# Transcrições alinhadas filtradas\n",
    "aligned_json_filtered_path = os.path.join(base_path, \"aligned_transcriptions_final.json\")\n",
    "\n",
    "\n",
    "# Lengenda Original\n",
    "srt_file_path = os.path.join(episode_path, episode_name, \"audio_file\", f\"{episode_name}.srt\")\n",
    "# Diairização\n",
    "diary_file_path = os.path.join(base_path, \"diarization_output.rttm\")\n",
    "# Transcrições alinhadas\n",
    "aligned_json_path = os.path.join(base_path, \"aligned_transcriptions_ready.json\")\n",
    "# Áudio original de vocais\n",
    "input_audio_path = os.path.join(base_path, \"vocals.wav\")\n",
    "# Diretório para salvar segmentos de áudio\n",
    "output_audio_dir = os.path.join(episode_path, episode_name, \"audio_file\", \"original_segments\")\n",
    "print_now(f\"\\nSegmentos originais salvos em: {output_audio_dir}\")\n",
    "\n",
    "# Carregar legendas\n",
    "subtitles = process_srt_to_json(srt_file_path)\n",
    "\n",
    "# Carregar segmentos de diarização\n",
    "with open(diary_file_path, 'r', encoding='utf-8') as diary_file:\n",
    "    diarization_segments = [\n",
    "        {\n",
    "            \"speaker\": line.split()[7],\n",
    "            \"start\": round(float(line.split()[3]), 3),\n",
    "            \"end\": round(float(line.split()[3]) + float(line.split()[4]), 3)\n",
    "        }\n",
    "        for line in diary_file.readlines() if line.strip()\n",
    "    ]\n",
    "\n",
    "# Validar alinhamento\n",
    "validated_segments = check_overlap(diarization_segments, subtitles)\n",
    "\n",
    "# Ajustar fragmentos\n",
    "final_segments = adjust_fragments(validated_segments)\n",
    "\n",
    "# Segmentar áudio e atualizar os segmentos com caminhos de áudio\n",
    "updated_segments = segment_audio(input_audio_path, final_segments, output_audio_dir, resume=True)\n",
    "\n",
    "# Salvar resultados em JSON\n",
    "with open(aligned_json_path, 'w', encoding='utf-8') as aligned_json:\n",
    "    json.dump(updated_segments, aligned_json, ensure_ascii=False, indent=4)\n",
    "\n",
    "print_now(f\"\\nAlinhamento concluído. Resultados salvos em {aligned_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celula 4 - Extração de Transcrições e Traduções: whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celula 4.1 - Extração de Transcrições e Traduções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extração de Transcrições e Traduções\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import ffmpeg\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#configurar print_now\n",
    "def print_now(message):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "# Carregar o modelo Whisper do Hugging Face\n",
    "\n",
    "def load_whisper_model_hf(model_name=\"openai/whisper-large-v3\", device=None):\n",
    "    \"\"\"\n",
    "    Carrega o modelo Whisper do Hugging Face com o processador e tokenizer associados.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nome do modelo do Hugging Face.\n",
    "        device (str): Dispositivo onde o modelo será carregado (\"cpu\" ou \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: Processador, modelo e dispositivo configurados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not device:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print_now(f\"Carregando o modelo Whisper ({model_name}) no dispositivo: {device}...\")\n",
    "\n",
    "        processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "        print_now(\"Modelo carregado com sucesso!\")\n",
    "        return processor, model, device\n",
    "    except Exception as e:\n",
    "        print_now(f\"Erro ao carregar o modelo Whisper: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Função para analisar arquivo RTTM\n",
    "\n",
    "def parse_rttm_file(rttm_path):\n",
    "    \"\"\"\n",
    "    Analisa um arquivo RTTM e retorna os segmentos em um formato estruturado.\n",
    "\n",
    "    Args:\n",
    "        rttm_path (str): Caminho para o arquivo RTTM.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de segmentos com informações de início, fim e locutor.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    try:\n",
    "        with open(rttm_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "\n",
    "                start_time = float(parts[3])\n",
    "                duration = float(parts[4])\n",
    "                end_time = start_time + duration\n",
    "                speaker = parts[7]\n",
    "\n",
    "                segments.append({\n",
    "                    \"start\": start_time,\n",
    "                    \"end\": end_time,\n",
    "                    \"speaker\": speaker\n",
    "                })\n",
    "\n",
    "        print_now(f\"Arquivo RTTM processado. {len(segments)} segmentos carregados.\")\n",
    "        return segments\n",
    "    except Exception as e:\n",
    "        print_now(f\"Erro ao analisar o arquivo RTTM: {e}\")\n",
    "        return []\n",
    "\n",
    "# Transcrever e traduzir segmentos com Hugging Face\n",
    "\n",
    "def transcribe_and_translate_hf(processor, model, device, audio_path, segments, target_language=\"pt\", save_temp=False, temp_dir=\"data/original_segments\", beam_size=5):\n",
    "    \"\"\"\n",
    "    Realiza transcrição e tradução utilizando o modelo Whisper do Hugging Face.\n",
    "\n",
    "    Args:\n",
    "        processor (WhisperProcessor): Processador do modelo Whisper.\n",
    "        model (WhisperForConditionalGeneration): Modelo Whisper carregado.\n",
    "        device (str): Dispositivo de execução (\"cpu\" ou \"cuda\").\n",
    "        audio_path (str): Caminho para o arquivo de áudio.\n",
    "        segments (list): Lista de segmentos com intervalos de tempo e IDs de locutores.\n",
    "        target_language (str): Idioma de destino para a tradução.\n",
    "        save_temp (bool): Se True, salva os arquivos de segmento temporários.\n",
    "        temp_dir (str): Diretório onde os segmentos temporários serão salvos.\n",
    "        beam_size (int): Tamanho do beam search para maior precisão.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de transcrições e traduções para cada segmento.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    errored_segments = []\n",
    "\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "\n",
    "    for segment in tqdm(segments, desc=\"Transcrevendo segmentos\"):\n",
    "        try:\n",
    "            start = float(segment[\"start\"])\n",
    "            end = float(segment[\"end\"])\n",
    "            duration = end - start\n",
    "\n",
    "            if end <= start:\n",
    "                raise ValueError(f\"Segmento inválido: end ({end}) <= start ({start})\")\n",
    "\n",
    "            speaker = segment[\"speaker\"]\n",
    "            temp_audio_path = os.path.join(temp_dir, f\"{speaker}_{start:.3f}-{duration:.3f}.wav\")\n",
    "\n",
    "            # Extração de segmento com FFmpeg\n",
    "            ffmpeg_command = (\n",
    "                f\"ffmpeg -y -i \\\"{audio_path}\\\" -ss {start} -to {end} -ar 16000 -ac 1 \\\"{temp_audio_path}\\\"\"\n",
    "            )\n",
    "            ffmpeg_result = os.system(ffmpeg_command)\n",
    "            if ffmpeg_result != 0:\n",
    "                raise RuntimeError(f\"FFmpeg falhou ao processar o segmento {start}-{end}.\")\n",
    "\n",
    "            # Preparar entrada para o modelo\n",
    "            audio_input, _ = librosa.load(temp_audio_path, sr=16000)\n",
    "            input_features = processor(audio_input, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "            # Gerar transcrição e tradução\n",
    "            generate_kwargs = {\n",
    "                \"language\": target_language,  # Define o idioma-alvo diretamente\n",
    "                \"num_beams\": beam_size,\n",
    "            }\n",
    "\n",
    "            attention_mask = torch.ones(input_features.shape, dtype=torch.long, device=device)\n",
    "            predicted_ids = model.generate(input_features, attention_mask=attention_mask, **generate_kwargs)\n",
    "\n",
    "            transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            print_now(f\"Transcrição para o segmento {segment}: {transcription}\")\n",
    "\n",
    "            # Adicionar resultados\n",
    "            results.append({\n",
    "                \"speaker\": speaker,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"transcription\": transcription\n",
    "            })\n",
    "\n",
    "            # Remover arquivo temporário, se configurado\n",
    "            if not save_temp:\n",
    "                os.remove(temp_audio_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print_now(f\"Erro ao processar o segmento {segment}: {e}\")\n",
    "            errored_segments.append(segment)\n",
    "\n",
    "    # Salvar resultados e segmentos com erro\n",
    "    output_path = os.path.join(temp_dir, \"transcriptions_results_hf.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "    print_now(f\"Resultados salvos em: {output_path}\")\n",
    "\n",
    "    if errored_segments:\n",
    "        error_log_path = os.path.join(temp_dir, \"errored_segments_hf.json\")\n",
    "        with open(error_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(errored_segments, f, ensure_ascii=False, indent=4)\n",
    "        print_now(f\"Segmentos com erro salvos em: {error_log_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exemplo de uso\n",
    "model_name = \"openai/whisper-large-v3\"\n",
    "audio_path = \"\"\n",
    "temp_dir = \"\"\n",
    "rttm_path = \"\"\n",
    "\n",
    "# Carregar modelo\n",
    "processor, model, device = load_whisper_model_hf(model_name)\n",
    "\n",
    "# Carregar segmentos do RTTM\n",
    "segments = parse_rttm_file(rttm_path)\n",
    "\n",
    "# Executar transcrição e tradução\n",
    "transcriptions = transcribe_and_translate_hf(processor, model, device, audio_path, segments, target_language=\"pt\", save_temp=True, temp_dir=temp_dir, beam_size=5)\n",
    "print_now(\"Transcrições e traduções completas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 4.2 - Salvar Transcrições e Traduções Formato Global(json e csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4.2 - Filtragem de Transcrições Vazias e salvamento dos resultados\n",
    "# Esta célula salva os resultados em JSON e CSV com os campos adicionais 'duration' e 'audio_data'.\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def filter_empty_transcriptions(results):\n",
    "    \"\"\"\n",
    "    Filtra os resultados para remover transcrições vazias.\n",
    "\n",
    "    Args:\n",
    "        results (list): Lista de dicionários com transcrições e traduções.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista filtrada sem transcrições vazias.\n",
    "    \"\"\"\n",
    "    filtered_results = [r for r in results if r.get(\"transcription\", \"\").strip()]\n",
    "    print(f\"Resultados filtrados: {len(filtered_results)} de {len(results)} segmentos válidos.\")\n",
    "    return filtered_results\n",
    "\n",
    "def save_results_with_duration_and_audio(results, json_path=\"results.json\", csv_path=\"results.csv\", audio_dir=\"data/audio_file/original_segments\"):\n",
    "    \"\"\"\n",
    "    Salva os resultados de transcrição em arquivos JSON e CSV com duração e caminho real dos arquivos de áudio.\n",
    "\n",
    "    Args:\n",
    "        results (list): Lista de dicionários com transcrições e traduções.\n",
    "        json_path (str): Caminho para o arquivo JSON.\n",
    "        csv_path (str): Caminho para o arquivo CSV.\n",
    "        audio_dir (str): Diretório onde os arquivos de áudio estão armazenados.\n",
    "    \"\"\"\n",
    "    for r in results:\n",
    "        # Calcula a duração e define o caminho real do arquivo de áudio\n",
    "        r[\"duration\"] = round(r[\"end\"] - r[\"start\"], 4)\n",
    "        r[\"audio_data\"] = os.path.join(audio_dir, f\"{r['speaker']}_{r['start']:.3f}-{r['duration']:.3f}.wav\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "    # Salvar em JSON\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Resultados salvos em JSON: {json_path}\")\n",
    "\n",
    "    # Salvar em CSV\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\"speaker\", \"start\", \"end\", \"duration\", \"transcription\", \"audio_data\"])\n",
    "        writer.writeheader()\n",
    "        for r in results:\n",
    "            writer.writerow(r)\n",
    "    print(f\"Resultados salvos em CSV: {csv_path}\")\n",
    "\n",
    "# Caminhos de saída\n",
    "json_path = \"\"\n",
    "csv_path = \"\"\n",
    "audio_dir = \"\"\n",
    "\n",
    "# Carregar os resultados do arquivo JSON\n",
    "transcriptions_result = \"\"\n",
    "with open(transcriptions_result, \"r\") as file:\n",
    "    transcriptions = json.load(file)\n",
    "\n",
    "# Filtragem e salvamento dos resultados\n",
    "results = filter_empty_transcriptions(transcriptions)\n",
    "save_results_with_duration_and_audio(results, json_path=json_path, csv_path=csv_path, audio_dir=audio_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificação de Arquivos de Áudio\n",
    "# import os\n",
    "import json\n",
    "\n",
    "def verify_audio_files(json_path, audio_dir):\n",
    "    \"\"\"\n",
    "    Verifica se todos os arquivos de áudio listados no JSON estão presentes no diretório especificado.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Caminho para o arquivo JSON com os dados das transcrições.\n",
    "        audio_dir (str): Diretório onde os arquivos de áudio devem estar.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Carregar transcrições do JSON\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcriptions = json.load(f)\n",
    "    \n",
    "    # Verificar a existência dos arquivos\n",
    "    missing_files = []\n",
    "    for transcription in transcriptions:\n",
    "        audio_path = transcription.get(\"audio_path\", \"\")\n",
    "        full_audio_path = os.path.join(audio_dir, os.path.basename(audio_path))\n",
    "        if not os.path.exists(full_audio_path):\n",
    "            missing_files.append(full_audio_path)\n",
    "\n",
    "    # Resultado\n",
    "    if missing_files:\n",
    "        print(f\"Arquivos ausentes ({len(missing_files)}):\")\n",
    "        for missing in missing_files:\n",
    "            print(missing)\n",
    "    else:\n",
    "        print(\"Todos os arquivos de áudio estão presentes.\")\n",
    "\n",
    "# Exemplo de uso\n",
    "json_path = \"\"\n",
    "audio_dir = \"\"\n",
    "verify_audio_files(json_path, audio_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Célula 5 - Geração de Áudio Personalizados por Locutor: TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 5.1 - Compilação de Trilhas Originais por Locutor Concatenadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5.1 - Compilação de Trilhas Originais por Locutor\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def compile_audio_by_speaker(segments, input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Compila trilhas de áudio originais separadas por locutor.\n",
    "\n",
    "    Args:\n",
    "        segments (list): Lista de segmentos com informações de locutores e tempos.\n",
    "        input_dir (str): Diretório contendo os arquivos de áudio segmentados.\n",
    "        output_dir (str): Diretório onde os arquivos compilados serão salvos.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    speaker_audio = {}\n",
    "\n",
    "    # Listar todos os arquivos reais no diretório para evitar erros de correspondência\n",
    "    existing_files = glob.glob(os.path.join(input_dir, \"*.wav\"))\n",
    "    existing_files = {os.path.basename(f): f for f in existing_files}\n",
    "\n",
    "    # Iterar pelos segmentos e agrupar por locutor\n",
    "    for segment in segments:\n",
    "        speaker = segment[\"speaker\"]\n",
    "        audio_path = segment[\"audio_path\"]  # Usar o caminho real do arquivo\n",
    "\n",
    "        # Verificar se o arquivo existe\n",
    "        if not os.path.exists(audio_path):\n",
    "            print(f\"[AVISO] Arquivo de áudio não encontrado: {audio_path}\")\n",
    "            continue\n",
    "\n",
    "        audio_segment = AudioSegment.from_wav(audio_path)\n",
    "\n",
    "        # Agrupar os segmentos por locutor\n",
    "        if speaker not in speaker_audio:\n",
    "            speaker_audio[speaker] = audio_segment\n",
    "        else:\n",
    "            speaker_audio[speaker] += audio_segment\n",
    "\n",
    "\n",
    "    # Exportar áudios compilados\n",
    "    for speaker, audio in speaker_audio.items():\n",
    "        output_path = os.path.join(output_dir, f\"{speaker}_compiled.wav\")\n",
    "        audio.export(output_path, format=\"wav\")\n",
    "        print(f\"Trilha compilada salva para {speaker}: {output_path}\")\n",
    "\n",
    "# Caminhos de entrada e saída\n",
    "audio_files_path = os.path.join(episode_path, episode_name, \"audio_file\")\n",
    "\n",
    "INPUT_DIR = f\"{audio_files_path}/original_segments\"\n",
    "OUTPUT_DIR = f\"{audio_files_path}/compiled_speaker_audio\"\n",
    "TRANSCRIPTIONS_PATH = \"\"\n",
    "\n",
    "# Carregar transcrições\n",
    "with open(TRANSCRIPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    segments = json.load(f)\n",
    "\n",
    "# Compilar áudios por locutor\n",
    "compile_audio_by_speaker(segments, INPUT_DIR, OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 5.2 - Teste de Geração direta com Vits / xtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5.2 - Geração Direta de Áudio com VITS Multspeaker\n",
    "# Atualização para lidar com o aviso do torch.load\n",
    "import functools\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Função para gerar áudio diretamente com VITS Multspeaker\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Ocultar Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#configurar print_now\n",
    "def print_now(message):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "def generate_audio_directly_with_vits(segments, tts_model, output_dir, compiled_audio_dir, language=\"pt\", speed=1.0):\n",
    "    \"\"\"\n",
    "    Gera áudio traduzido diretamente com IDs de locutores e transcrições usando VITS,\n",
    "    ou insere arquivos de silêncio para transcrições ausentes ou inválidas.\n",
    "\n",
    "    Args:\n",
    "        segments (list): Lista de segmentos com transcrições e IDs de locutores.\n",
    "        tts_model (TTS): Instância do modelo VITS carregado.\n",
    "        output_dir (str): Diretório para salvar os arquivos de áudio gerados.\n",
    "        audio_dir (str): Diretório contendo os arquivos de áudio de entrada por segmento.\n",
    "        compiled_audio_dir (str): Diretório contendo os arquivos compilados de cada locutor.\n",
    "        language (str): Idioma alvo para a geração de áudio.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de caminhos dos arquivos de áudio gerados.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    MIN_DURATION = 0.3  # duração mínima em segundos\n",
    "    generated_files = []\n",
    "\n",
    "    for segment in segments:\n",
    "        speaker = segment[\"speaker\"]\n",
    "        start = segment[\"start\"]\n",
    "        duration = segment[\"duration\"]\n",
    "        #transcription = segment.get(\"transcription\", \"\").strip()\n",
    "        transcription = segment[\"text\"]\n",
    "\n",
    "        # Definir caminho do áudio compilado do locutor\n",
    "        #speaker_wav = os.path.join(compiled_audio_dir, f\"{speaker}_compiled.wav\")\n",
    "        speaker_wav = segment[\"audio_path\"]\n",
    "\n",
    "        if not os.path.exists(speaker_wav):\n",
    "            print_now(f\"[ERRO] Arquivo do segmento {speaker_wav} não encontrado. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        # Caminho para o áudio de saída\n",
    "        output_path = os.path.join(output_dir, f\"{speaker}_{start:.3f}-{duration:.3f}.wav\")\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            print_now(f\"[INFO] Arquivo já gerado: {output_path}. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Verificar duração mínima\n",
    "            if duration < MIN_DURATION:\n",
    "                print_now(f\"[INFO] Segmento muito curto para {speaker} ({start:.3f}-{duration:.3f}). Gerando silêncio...\")\n",
    "                duration_ms = duration * 1000\n",
    "                silence = AudioSegment.silent(duration=duration_ms)\n",
    "                silence.export(output_path, format=\"wav\")\n",
    "                generated_files.append(output_path)\n",
    "                continue\n",
    "\n",
    "            # Validar transcrição\n",
    "            if transcription:\n",
    "                print_now(f\"Gerando áudio para locutor {speaker}, segmento {start:.3f}-{duration:.3f}...\")\n",
    "                tts_model.tts_to_file(\n",
    "                    text=transcription,\n",
    "                    speaker_wav=speaker_wav,\n",
    "                    language=language,\n",
    "                    file_path=output_path,\n",
    "                    speed=speed,\n",
    "                    split_sentences=False, # Se False, consome mais memória\n",
    "                )\n",
    "                print_now(f\"Áudio gerado com sucesso para {speaker}, salvo em: {output_path}\")\n",
    "            else:\n",
    "                print_now(f\"[INFO] Transcrição inválida ou ausente para {speaker}, segmento {start:.3f}-{duration:.3f}. Gerando silêncio...\")\n",
    "                duration_ms = duration * 1000\n",
    "                silence = AudioSegment.silent(duration=duration_ms)\n",
    "                silence.export(output_path, format=\"wav\")\n",
    "\n",
    "            generated_files.append(output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print_now(f\"Erro ao processar segmento {speaker} ({start:.3f}-{duration:.3f}): {e}\")\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "# Caminhos e inicialização\n",
    "LANGUAGE = \"pt\"\n",
    "SPEED = 1.8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "audio_files_path = os.path.join(episode_path, episode_name, \"audio_file\")\n",
    "\n",
    "TRANSCRIPTIONS_PATH = \"\"\n",
    "AUDIO_OUTPUT_DIR = \"\"\n",
    "COMPILED_AUDIO_DIR = \"\"\n",
    "\n",
    "# Carregar transcrições\n",
    "with open(TRANSCRIPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    segments = json.load(f)\n",
    "\n",
    "# Inicializar o modelo VITS\n",
    "MODEL_NAME = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
    "print_now(\"Carregando modelo VITS...\")\n",
    "tts = TTS(model_name=MODEL_NAME).to(DEVICE)\n",
    "print_now(f\"Modelo carregado com sucesso no dispositivo: {DEVICE}\")\n",
    "\n",
    "# Gerar áudios diretamente com VITS\n",
    "print_now(\"Iniciando a geração direta de áudio com VITS...\")\n",
    "generated_files = generate_audio_directly_with_vits(segments, tts, AUDIO_OUTPUT_DIR, COMPILED_AUDIO_DIR, LANGUAGE, SPEED)\n",
    "\n",
    "print_now(\"Processo de geração concluído!\")\n",
    "print_now(f\"Total de arquivos gerados: {len(generated_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Célula 6 - Mixagem de Aúdio e Vídeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem da Linha do Tempo\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Caminho do arquivo JSON\n",
    "aligned_json_path = \"\"\n",
    "\n",
    "# Carregar os dados do JSON\n",
    "with open(aligned_json_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Configurar a linha do tempo com tamanho ajustável\n",
    "def plot_timeline(figsize=(12, 6)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Criar o gráfico de segmentos\n",
    "    speakers = set(segment['speaker'] for segment in data)\n",
    "    colors = plt.cm.tab10.colors  # Paleta de cores\n",
    "    speaker_colors = {speaker: colors[i % len(colors)] for i, speaker in enumerate(speakers)}\n",
    "\n",
    "    for segment in data:\n",
    "        start = segment['start']\n",
    "        end = segment['end']\n",
    "        duration = segment['duration']\n",
    "        speaker = segment['speaker']\n",
    "        ax.broken_barh([(start, duration)], (int(speaker.split('_')[1]) * 10, 9), color=speaker_colors[speaker], label=speaker)\n",
    "\n",
    "    # Configurar eixos\n",
    "    ax.set_xlabel(\"Tempo (MM:SS)\")\n",
    "    ax.set_title(\"Linha do Tempo dos Segmentos de Áudio\")\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Remover legenda do eixo Y\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    # Formatar o eixo X para MM:SS\n",
    "    def seconds_to_mmss(x, pos):\n",
    "        minutes = int(x // 60)\n",
    "        seconds = int(x % 60)\n",
    "        return f\"{minutes:02}:{seconds:02}\"\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(seconds_to_mmss))\n",
    "\n",
    "    # Legenda\n",
    "    handles = [mpatches.Patch(color=color, label=speaker) for speaker, color in speaker_colors.items()]\n",
    "    ax.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Locutores\")\n",
    "\n",
    "    # Mostrar gráfico\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Chamada para plotar com tamanho ajustável\n",
    "plot_timeline(figsize=(16, 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 6.1 - Criação da Trilha Final de Áudio Dublado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6.1 - Criação da Trilha Final de Áudio Dublado\n",
    "\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def calculate_total_duration(audio_path):\n",
    "    \"\"\"\n",
    "    Calcula a duração total do áudio original.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Caminho para o arquivo de áudio original.\n",
    "\n",
    "    Returns:\n",
    "        float: Duração total do áudio em segundos.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    return len(audio) / 1000.0  # Retorna em segundos\n",
    "\n",
    "def create_final_audio_track(segment_dir, output_path, audio_path):\n",
    "    \"\"\"\n",
    "    Cria uma trilha de áudio alinhada com o tempo total da trilha original, \n",
    "    inserindo os segmentos dublados nos tempos corretos.\n",
    "\n",
    "    Args:\n",
    "        segment_dir (str): Diretório contendo os segmentos dublados.\n",
    "        output_path (str): Caminho para salvar a trilha final.\n",
    "        audio_path (str): Caminho para o áudio original.\n",
    "\n",
    "    Returns:\n",
    "        str: Caminho da trilha final criada.\n",
    "    \"\"\"\n",
    "    # Calcular a duração total do áudio original\n",
    "    total_duration = calculate_total_duration(audio_path)\n",
    "\n",
    "    # Criar uma trilha vazia com a duração total\n",
    "    final_track = AudioSegment.silent(duration=total_duration * 1000)  # Convertendo para milissegundos\n",
    "\n",
    "    # Iterar pelos arquivos de segmentos dublados\n",
    "    for segment_file in os.listdir(segment_dir):\n",
    "        if segment_file.endswith(\".wav\"):\n",
    "            # Extrair informações do nome do arquivo\n",
    "            try:\n",
    "                speaker, start_duration = segment_file.rsplit(\"_\", 1)\n",
    "                start, duration = map(float, start_duration.replace(\".wav\", \"\").split(\"-\"))\n",
    "            except ValueError:\n",
    "                print(f\"[ERRO] Nome de arquivo inválido: {segment_file}. Pulando...\")\n",
    "                continue\n",
    "\n",
    "            # Carregar o segmento de áudio\n",
    "            segment_path = os.path.join(segment_dir, segment_file)\n",
    "            segment_audio = AudioSegment.from_wav(segment_path)\n",
    "\n",
    "            # Calcular a posição inicial na trilha final\n",
    "            start_time_ms = int(start * 1000)  # Convertendo para milissegundos\n",
    "\n",
    "            # Inserir o segmento na trilha final\n",
    "            final_track = final_track.overlay(segment_audio, position=start_time_ms)\n",
    "            print(f\"[INFO] Segmento {segment_file} inserido em {start_time_ms} ms.\")\n",
    "\n",
    "    # Salvar a trilha final\n",
    "    final_track.export(output_path, format=\"wav\")\n",
    "    print(f\"Trilha final criada e salva em: {output_path}\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Caminhos e inicialização\n",
    "LANGUAGE = \"pt\"\n",
    "SPEED = 1.8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "audio_files_path = os.path.join(episode_path, episode_name, \"audio_file\")\n",
    "\n",
    "# Caminhos\n",
    "SEGMENT_DIR = f\"{audio_files_path}/generated_audio_condicional_{LANGUAGE}_{SPEED}\"\n",
    "OUTPUT_PATH = f\"{audio_files_path}/final_audio_track.wav\"\n",
    "AUDIO_PATH = f\"{audio_files_path}/{episode_name}.wav\"\n",
    "\n",
    "# Criar a trilha final\n",
    "create_final_audio_track(SEGMENT_DIR, OUTPUT_PATH, AUDIO_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 6.2 - Mixagem Final das Trilhas de Áudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6.2 - Mixagem Final das Trilhas de Áudio\n",
    "import os\n",
    "from pydub import AudioSegment, effects\n",
    "from moviepy import VideoFileClip, AudioFileClip\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    \"\"\"\n",
    "    Normaliza o volume de uma faixa de áudio.\n",
    "\n",
    "    Args:\n",
    "        audio (AudioSegment): Faixa de áudio a ser normalizada.\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: Faixa de áudio normalizada.\n",
    "    \"\"\"\n",
    "    return effects.normalize(audio)\n",
    "\n",
    "def mix_audio_tracks(audio_paths, output_path):\n",
    "    \"\"\"\n",
    "    Realiza a mixagem das faixas de áudio especificadas.\n",
    "\n",
    "    Args:\n",
    "        audio_paths (list): Lista de caminhos para as faixas de áudio.\n",
    "        output_path (str): Caminho para salvar a trilha mixada.\n",
    "\n",
    "    Returns:\n",
    "        str: Caminho da trilha mixada.\n",
    "    \"\"\"\n",
    "    if not audio_paths:\n",
    "        raise ValueError(\"Nenhum caminho de áudio fornecido para mixagem.\")\n",
    "\n",
    "    # Carregar e normalizar as faixas de áudio\n",
    "    tracks = []\n",
    "    for path in audio_paths:\n",
    "        print(f\"Carregando e normalizando faixa: {path}\")\n",
    "        track = AudioSegment.from_wav(path)\n",
    "        tracks.append(normalize_audio(track))\n",
    "\n",
    "    # Mixar as faixas de áudio\n",
    "    mixed_audio = tracks[0]\n",
    "    for track in tracks[1:]:\n",
    "        mixed_audio = mixed_audio.overlay(track)\n",
    "\n",
    "    # Salvar a trilha mixada\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    mixed_audio.export(output_path, format=\"wav\")\n",
    "    print(f\"Trilha mixada salva em: {output_path}\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def replace_audio_in_video(video_path, audio_path, output_video_path):\n",
    "    \"\"\"\n",
    "    Substitui o áudio original de um vídeo pela trilha mixada.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Caminho para o vídeo original.\n",
    "        audio_path (str): Caminho para a trilha de áudio mixada.\n",
    "        output_video_path (str): Caminho para salvar o vídeo com o áudio substituído.\n",
    "\n",
    "    Returns:\n",
    "        str: Caminho do vídeo final.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
    "    print(f\"Substituindo áudio no vídeo: {video_path}\")\n",
    "    video = VideoFileClip(video_path)\n",
    "    new_audio = AudioFileClip(audio_path)\n",
    "    video.audio = new_audio\n",
    "    #video = video.set_audio(new_audio)\n",
    "    video.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "    print(f\"Vídeo final salvo em: {output_video_path}\")\n",
    "\n",
    "    return output_video_path\n",
    "\n",
    "\n",
    "# Caminhos\n",
    "BASS_PATH = \".../bass.wav\"\n",
    "DRUMS_PATH = \".../drums.wav\"\n",
    "OTHER_PATH = \".../other.wav\"\n",
    "DUBBED_PATH = \"\"\n",
    "FINAL_AUDIO_PATH = \"\"\n",
    "VIDEO_PATH = \"\"\n",
    "FINAL_VIDEO_PATH = \"\"\n",
    "\n",
    "# Mixar as faixas de áudio\n",
    "AUDIO_PATHS = [BASS_PATH, DRUMS_PATH, OTHER_PATH, DUBBED_PATH]\n",
    "mix_audio_tracks(AUDIO_PATHS, FINAL_AUDIO_PATH)\n",
    "\n",
    "# Substituir o áudio original do vídeo\n",
    "replace_audio_in_video(VIDEO_PATH, FINAL_AUDIO_PATH, FINAL_VIDEO_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 6.3 - Adicionar Legendas ao Vídeo com MoviePy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6.1.5 - Adicionar Legendas ao Vídeo com MoviePy\n",
    "# Esta célula adiciona legendas de um arquivo .srt ao vídeo original.\n",
    "\n",
    "# Instalando dependências necessárias\n",
    "# pip install moviepy pysrt\n",
    "\n",
    "from moviepy import VideoFileClip\n",
    "import subprocess\n",
    "\n",
    "def add_subtitles_to_video(video_path, subtitles_path, output_path):\n",
    "    \"\"\"\n",
    "    Adiciona legendas ao vídeo usando um arquivo .srt.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Caminho para o arquivo de vídeo original.\n",
    "        subtitles_path (str): Caminho para o arquivo de legendas .srt.\n",
    "        output_path (str): Caminho para o vídeo de saída com legendas.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Verificando dependência do FFmpeg...\")\n",
    "        ffmpeg_installed = subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if ffmpeg_installed.returncode != 0:\n",
    "            raise EnvironmentError(\"FFmpeg não está instalado ou não está disponível no PATH.\")\n",
    "\n",
    "        print(\"Iniciando adição de legendas...\")\n",
    "        command = (\n",
    "            f\"ffmpeg -i {video_path} -vf subtitles={subtitles_path} -c:v libx264 -c:a copy {output_path}\"\n",
    "        )\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "        print(f\"Vídeo com legendas gerado em: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao adicionar legendas ao vídeo: {e}\")\n",
    "\n",
    "# Exemplo de uso:\n",
    "add_subtitles_to_video(\n",
    "    video_path=\"\",\n",
    "    subtitles_path=\"\",\n",
    "    output_path=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Célula 7- Fine-Tuning com Xtts_V2 e GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Metadata CSV\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Criar metadata.csv\n",
    "def create_and_save_metadata_csv(json_path, csv_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=\"|\")\n",
    "        for entry in data:\n",
    "            writer.writerow([\n",
    "                entry[\"audio_data\"],  # Caminho do arquivo de áudio\n",
    "                entry[\"transcription\"],  # Texto da transcrição\n",
    "                entry.get(\"speaker\", \"custom_speaker\")  # Nome do locutor\n",
    "            ])\n",
    "    print(f\"Arquivo {csv_path} criado com sucesso!\")\n",
    "\n",
    "TRANSCRIPTIONS_PATH = \"\"\n",
    "METADATA_CSV_PATH = \"\"\n",
    "\n",
    "create_and_save_metadata_csv(TRANSCRIPTIONS_PATH, METADATA_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Célula 7.1 - Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar arquivos de áudio\n",
    "def verify_audio_files(manifest_file, dataset_path):\n",
    "    issues = []\n",
    "    print(\"Verificando arquivos de áudio...\")\n",
    "    with open(manifest_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols = line.strip().split(\"|\")\n",
    "            audio_file = os.path.join(cols[0])  # Caminho do arquivo de áudio\n",
    "            if not os.path.isfile(audio_file):\n",
    "                issues.append(f\"Arquivo não encontrado: {audio_file}\")\n",
    "                continue\n",
    "            try:\n",
    "                # Carregar o áudio\n",
    "                waveform, sample_rate = torchaudio.load(audio_file)\n",
    "                if sample_rate != 22050:\n",
    "                    issues.append(f\"Taxa de amostragem inesperada em {audio_file}: {sample_rate} Hz\")\n",
    "                if waveform.abs().max() > 1.0:\n",
    "                    issues.append(f\"Amplitude fora do intervalo esperado [-1, 1] em {audio_file}\")\n",
    "            except Exception as e:\n",
    "                issues.append(f\"Erro ao carregar {audio_file}: {e}\")\n",
    "    if not issues:\n",
    "        print(\"Todos os arquivos estão no formato esperado!\")\n",
    "    else:\n",
    "        print(\"Problemas encontrados:\")\n",
    "        for issue in issues:\n",
    "            print(f\" - {issue}\")\n",
    "\n",
    "# Exemplo de uso para verificar dados\n",
    "manifest_file = \".../metadata.csv\"  # Caminho para o arquivo de metadados\n",
    "dataset_path = \"\"  # Caminho para os arquivos de áudio\n",
    "verify_audio_files(manifest_file, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7.1 - Configuração e Treinamento do Modelo VITS\n",
    "# Train TRAIN_GPT_XTTS.py\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar dispositivo\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#configurar print_now\n",
    "def print_now(message):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "# Logging parameters\n",
    "RUN_NAME = \"GPT_XTTS_v2.0_Custom\"\n",
    "PROJECT_NAME = \"XTTS_trainer\"\n",
    "RUN_DESCRIPTION = \"Fine_tuning.\"\n",
    "DASHBOARD_LOGGER = \"tensorboard\"\n",
    "LOGGER_URI = None\n",
    "\n",
    "# Formatter customizado\n",
    "def custom_formatter(root_path, manifest_file, **kwargs):\n",
    "    \"\"\"Formatter customizado para o nosso metadata.csv.\"\"\"\n",
    "    txt_file = os.path.join(root_path, manifest_file)\n",
    "    items = []\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n",
    "        for line in ttf:\n",
    "            cols = line.strip().split(\"|\")\n",
    "            wav_file = os.path.normpath(cols[0])  # Usa o caminho direto do arquivo\n",
    "            if not os.path.isfile(wav_file):\n",
    "                raise FileNotFoundError(f\"Arquivo de áudio não encontrado: {wav_file}\")\n",
    "            text = cols[1]\n",
    "            speaker_name = cols[2]\n",
    "            items.append({\n",
    "                \"text\": text,\n",
    "                \"audio_file\": wav_file,\n",
    "                \"speaker_name\": speaker_name,\n",
    "                \"root_path\": root_path\n",
    "            })\n",
    "    return items\n",
    "\n",
    "# Set here the path that the checkpoints will be saved. Default: ./run/training/\n",
    "OUT_PATH = os.path.join(\"model_new\", \"run\", \"training\")\n",
    "DATASET_PATH = \"data/audio_file/original_segments\"\n",
    "METADATA_PATH = os.path.join(\"metadata.csv\")\n",
    "\n",
    "# Training Parameters\n",
    "EPOCHS = 20  # set here the number of epochs\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False\n",
    "START_WITH_EVAL = True  # if True it will star with evaluation\n",
    "BATCH_SIZE = 3  # set here the batch size\n",
    "GRAD_ACUMM_STEPS = 84  # set here the grad accumulation steps\n",
    "# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training. You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly.\n",
    "\n",
    "# Define here the dataset that you want to use for the fine-tuning on.\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"custom_formatter\",\n",
    "    dataset_name=\"my_dataset\",\n",
    "    path=DATASET_PATH,\n",
    "    meta_file_train=METADATA_PATH,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "# Add here the configs of the datasets\n",
    "DATASETS_CONFIG_LIST = [config_dataset]\n",
    "\n",
    "# Define the path where XTTS v2.0.1 files will be downloaded\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# DVAE files\n",
    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# download DVAE files if needed\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print_now(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "\n",
    "# Download XTTS v2.0 checkpoint if needed\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n",
    "\n",
    "# download XTTS v2.0 files if needed\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print_now(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n",
    "    )\n",
    "\n",
    "LANGUAGE = config_dataset.language\n",
    "\n",
    "#def main():\n",
    "# init args and config\n",
    "print_now(\"Inicilizando Configurações...\")\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=132300,  # 6 secs\n",
    "    min_conditioning_length=66150,  # 3 secs\n",
    "    debug_loading_failures=False,\n",
    "    max_wav_length=255995,  # ~11.6 seconds\n",
    "    max_text_length=200,\n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    gpt_num_audio_tokens=1026,\n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")\n",
    "# define audio config\n",
    "audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n",
    "# training parameters config\n",
    "config = GPTTrainerConfig(\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    run_description=RUN_DESCRIPTION,\n",
    "    epochs=EPOCHS,\n",
    "    dashboard_logger=DASHBOARD_LOGGER,\n",
    "    logger_uri=LOGGER_URI,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=8,\n",
    "    eval_split_size=0.1,\n",
    "    eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=10000,\n",
    "    save_n_checkpoints=1,\n",
    "    save_checkpoints=True,\n",
    "    # target_loss=\"loss\",\n",
    "    print_eval=False,\n",
    "    # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=5e-06,  # learning rate\n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    # it was adjusted accordly for the new step scheme\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    mixed_precision=False,\n",
    ")\n",
    "print_now(\"Configurações inicializadas.\")\n",
    "\n",
    "# init the model from config\n",
    "print_now(\"Inicializando o Modelo...\")\n",
    "model = GPTTrainer.init_from_config(config)\n",
    "#model = model.to(DEVICE)\n",
    "print_now(\"Modelo inicializado.\")\n",
    "\n",
    "# load training samples\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    DATASETS_CONFIG_LIST,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    "    formatter=custom_formatter,\n",
    ")\n",
    "print_now(f\"Quantidade de Amostras de Treino: {len(train_samples)}, Quantidade de Amostras de Validação: {len(eval_samples)}\")\n",
    "print_now(\"Amostras de treino e validação carregadas.\")\n",
    "\n",
    "# init the trainer and 🚀\n",
    "print_now(\"Iniciando o treinamento...\")\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=START_WITH_EVAL,\n",
    "        grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")\n",
    "trainer.fit()\n",
    "print_now(\"Treinamento concluído.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celula 7.2 - Inferência "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7.2 - Configuração e Treinamento do Modelo VITS Default Script\n",
    "# Inferência - TRAIN_GPT_XTTS.py\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "\n",
    "# Caminhos para os arquivos necessários\n",
    "VOCAB_PATH = \".../vocab.json\"\n",
    "CONFIG_PATH = \".../config.json\"\n",
    "BEST_MODEL = \".../best_model.pth\"\n",
    "METADATA_FILE = \".../metadata.csv\"\n",
    "OUTPUT_DIR = \"\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Função para impressão com timestamp\n",
    "def print_now(message):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "# Carregar o modelo\n",
    "def load_model(config_path, model_path, vocab_path):\n",
    "    print_now(\"Carregando modelo e configuração para inferência...\")\n",
    "    if not os.path.isfile(config_path):\n",
    "        raise FileNotFoundError(f\"Configuração não encontrada: {config_path}\")\n",
    "    if not os.path.isfile(model_path):\n",
    "        raise FileNotFoundError(f\"Modelo não encontrado: {model_path}\")\n",
    "    if not os.path.isfile(vocab_path):\n",
    "        raise FileNotFoundError(f\"Arquivo de vocabulário não encontrado: {vocab_path}\")\n",
    "\n",
    "    config = XttsConfig()\n",
    "    config.load_json(config_path)\n",
    "    model = Xtts.init_from_config(config)\n",
    "    model.load_checkpoint(config, checkpoint_path=model_path, vocab_path=vocab_path, use_deepspeed=False)\n",
    "    model.cuda()\n",
    "    print_now(\"Modelo carregado com sucesso.\")\n",
    "    return model\n",
    "\n",
    "# Realizar inferência\n",
    "def run_inference(model, metadata_file, output_dir):\n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as meta:\n",
    "        for line in meta:\n",
    "            cols = line.strip().split(\"|\")\n",
    "            audio_file = cols[0]\n",
    "            text = cols[1]\n",
    "            speaker_name = cols[2]\n",
    "\n",
    "            output_file = os.path.join(output_dir, f\"{os.path.basename(audio_file).replace('.wav', '_synthesized.wav')}\")\n",
    "\n",
    "            print_now(f\"Sintetizando: {text} para locutor: {speaker_name}\")\n",
    "\n",
    "            # Inferência\n",
    "            gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=[audio_file])\n",
    "            out = model.inference(text, \"en\", gpt_cond_latent, speaker_embedding, temperature=0.7)\n",
    "\n",
    "            # Salvar áudio sintetizado\n",
    "            torchaudio.save(output_file, torch.tensor(out[\"wav\"]).unsqueeze(0), 24000)\n",
    "            print_now(f\"Áudio sintetizado salvo em: {output_file}\")\n",
    "\n",
    "# Inferência principal\n",
    "def main():\n",
    "    model = load_model(CONFIG_PATH, BEST_MODEL, VOCAB_PATH)\n",
    "    run_inference(model, METADATA_FILE, OUTPUT_DIR)\n",
    "    print_now(\"Inferência concluída.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anidub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
